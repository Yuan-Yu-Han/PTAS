{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuan-Yu-Han/PTAS/blob/main/machine_translation_using_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43eb9577",
      "metadata": {
        "papermill": {
          "duration": 0.006891,
          "end_time": "2025-07-06T09:16:55.133831",
          "exception": false,
          "start_time": "2025-07-06T09:16:55.126940",
          "status": "completed"
        },
        "tags": [],
        "id": "43eb9577"
      },
      "source": [
        "## Machine Translation using transformers: Overview\n",
        "\n",
        "Transformers are a groundbreaking neural network architecture that has significantly advanced the field of natural language processing (NLP), particularly in sequence-to-sequence tasks. They've become the backbone of many modern NLP applications, including machine translation, text summarization, and language modeling.\n",
        "\n",
        "A defining feature of transformers is their ability to process entire input sequences simultaneously. Unlike traditional recurrent models, which handle one token at a time, transformers leverage a self-attention mechanism to analyze relationships between all elements in a sequence at once. This enables the model to focus on relevant parts of the input context more efficiently and with greater flexibility.\n",
        "\n",
        "In this tutorial, we'll explore how to build a machine translation system using the transformer architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e06acb4c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T09:16:55.147033Z",
          "iopub.status.busy": "2025-07-06T09:16:55.146625Z",
          "iopub.status.idle": "2025-07-06T09:19:20.776469Z",
          "shell.execute_reply": "2025-07-06T09:19:20.775269Z"
        },
        "papermill": {
          "duration": 145.639015,
          "end_time": "2025-07-06T09:19:20.778783",
          "exception": false,
          "start_time": "2025-07-06T09:16:55.139768",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06acb4c",
        "outputId": "89e91b59-996d-429b-89cd-eb77cad9b70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping torchtune as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext==0.18.0) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2026.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.12/dist-packages (3.2.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "# ✅ Install required packages with compatible versions for PyTorch-based English-to-French translation.\n",
        "# Includes torch (with GPU support), torchtext for dataset utilities, spaCy for tokenization,\n",
        "# and tqdm for progress visualization.\n",
        "\n",
        "\n",
        "# Remove torchtune (if exist)\n",
        "!pip uninstall torchtune -y\n",
        "\n",
        "# Reinstall compatible versions\n",
        "!pip install torch==2.3.0 torchvision torchaudio\n",
        "!pip install torchtext==0.18.0 torchdata\n",
        "!pip install 'portalocker>=2.0.0' --no-deps\n",
        "!pip install numpy==1.26.4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dad5bac1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:46:40.550656Z",
          "iopub.status.busy": "2025-07-06T08:46:40.549956Z",
          "iopub.status.idle": "2025-07-06T08:46:45.129667Z",
          "shell.execute_reply": "2025-07-06T08:46:45.128747Z",
          "shell.execute_reply.started": "2025-07-06T08:46:40.550629Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dad5bac1",
        "outputId": "15b090c0-6852-4d08-eeb2-377fb8831997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gputil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0bae9852",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:46:47.675277Z",
          "iopub.status.busy": "2025-07-06T08:46:47.674715Z",
          "iopub.status.idle": "2025-07-06T08:46:49.875546Z",
          "shell.execute_reply": "2025-07-06T08:46:49.874917Z",
          "shell.execute_reply.started": "2025-07-06T08:46:47.675244Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0bae9852",
        "outputId": "98f5e953-4822-470c-cde2-e30aeb460034"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Confirm the torch version\n",
        "\n",
        "import torch\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4dc5c5f",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "d4dc5c5f"
      },
      "source": [
        "### Loading The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ad0be9b2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:46:52.132568Z",
          "iopub.status.busy": "2025-07-06T08:46:52.132138Z",
          "iopub.status.idle": "2025-07-06T08:46:54.014034Z",
          "shell.execute_reply": "2025-07-06T08:46:54.013505Z",
          "shell.execute_reply.started": "2025-07-06T08:46:52.132545Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad0be9b2",
        "outputId": "71c26ea4-5bd9-4901-c91e-af0f7c47cc04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9827016a",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "9827016a"
      },
      "source": [
        "##### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f1222cb6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:46:56.058978Z",
          "iopub.status.busy": "2025-07-06T08:46:56.058174Z",
          "iopub.status.idle": "2025-07-06T08:46:56.423357Z",
          "shell.execute_reply": "2025-07-06T08:46:56.422716Z",
          "shell.execute_reply.started": "2025-07-06T08:46:56.058951Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "f1222cb6",
        "outputId": "847547dd-3e0b-40ba-b20e-0f1955ba53e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Collecting kagglehub\n",
            "  Downloading kagglehub-0.4.2-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub)\n",
            "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kagglesdk<1.0,>=0.1.14->kagglehub) (5.29.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2026.1.4)\n",
            "Downloading kagglehub-0.4.2-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.3/69.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kagglesdk, kagglehub\n",
            "  Attempting uninstall: kagglehub\n",
            "    Found existing installation: kagglehub 0.3.13\n",
            "    Uninstalling kagglehub-0.3.13:\n",
            "      Successfully uninstalled kagglehub-0.3.13\n",
            "Successfully installed kagglehub-0.4.2 kagglesdk-0.1.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "kagglehub"
                ]
              },
              "id": "c43f71a3558944afa81f74f849aea5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'english-to-french' dataset.\n",
            "Path to dataset files: /kaggle/input/english-to-french\n"
          ]
        }
      ],
      "source": [
        "!pip install -U kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"harishreddy18/english-to-french\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "81c51d8d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:46:59.074858Z",
          "iopub.status.busy": "2025-07-06T08:46:59.074580Z",
          "iopub.status.idle": "2025-07-06T08:46:59.582977Z",
          "shell.execute_reply": "2025-07-06T08:46:59.582184Z",
          "shell.execute_reply.started": "2025-07-06T08:46:59.074839Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c51d8d",
        "outputId": "3c0de2c1-c4d2-479b-dce7-ab889e12d7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['new jersey is sometimes quiet during autumn ', 'the united states is usually chilly during july ', 'california is usually quiet during march ', 'the united states is sometimes mild during june ', 'your least liked fruit is the grape ', 'his favorite fruit is the orange ', 'paris is relaxing during december ']\n",
            "[\"new jersey est parfois calme pendant l' automne \", 'les états-unis est généralement froid en juillet ', 'california est généralement calme en mars ', 'les états-unis est parfois légère en juin ', 'votre moins aimé fruit est le raisin ', \"son fruit préféré est l'orange \", 'paris est relaxant en décembre ']\n"
          ]
        }
      ],
      "source": [
        "en_df = pd.read_csv('/kaggle/input/english-to-french/small_vocab_en.csv', header=None, usecols=[0])\n",
        "fr_df = pd.read_csv('/kaggle/input/english-to-french/small_vocab_fr.csv', header=None, usecols=[0])\n",
        "\n",
        "english_sentences = en_df[0].astype(str).tolist()\n",
        "french_sentences = fr_df[0].astype(str).tolist()\n",
        "\n",
        "print(english_sentences[0:7])\n",
        "print(french_sentences[0:7])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b259dfb",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "4b259dfb"
      },
      "source": [
        "### Vocabulary Building and Sentence Encoding\n",
        "\n",
        "This section defines helper functions and builds vocabularies for the English (source) and French (target) datasets.\n",
        "\n",
        "1. **`yield_tokens`**: A generator function that tokenizes each sentence by lowercasing and splitting on spaces.\n",
        "2. **Vocabulary Creation**:\n",
        "   - Uses `build_vocab_from_iterator` to create source (`src_vocab`) and target (`trg_vocab`) vocabularies from tokenized English and French sentences.\n",
        "   - Adds special tokens: `<pad>` for padding, `<sos>` (start of sentence), and `<eos>` (end of sentence).\n",
        "   - Sets the default index to the padding token (`<pad>`), which is mapped to index 0.\n",
        "3. **`encode_sentence`**:\n",
        "   - Converts a raw sentence into a list of token indices using the corresponding vocabulary.\n",
        "   - Optionally prepends `<sos>` and appends `<eos>` tokens for use in sequence models.\n",
        "   - Returns the encoded sentence as a PyTorch tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "09869a74",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:00.951599Z",
          "iopub.status.busy": "2025-07-06T08:47:00.950831Z",
          "iopub.status.idle": "2025-07-06T08:47:01.607436Z",
          "shell.execute_reply": "2025-07-06T08:47:01.606624Z",
          "shell.execute_reply.started": "2025-07-06T08:47:00.951572Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "09869a74"
      },
      "outputs": [],
      "source": [
        "def yield_tokens(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield sentence.strip().lower().split()\n",
        "\n",
        "SRC_PAD_IDX = 0\n",
        "TRG_PAD_IDX = 0\n",
        "\n",
        "src_vocab = build_vocab_from_iterator(yield_tokens(english_sentences), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "src_vocab.set_default_index(SRC_PAD_IDX) # if a token is not found in vocab set it to 0.\n",
        "\n",
        "trg_vocab = build_vocab_from_iterator(yield_tokens(french_sentences), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
        "trg_vocab.set_default_index(TRG_PAD_IDX) # if a token is not found in vocab set it to 0.\n",
        "\n",
        "def encode_sentence(sentence, vocab, add_specials=True):\n",
        "    tokens = sentence.lower().strip().split()\n",
        "    if add_specials:\n",
        "        tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    return torch.tensor(vocab(tokens), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6412b47",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "a6412b47"
      },
      "source": [
        "### Custom Dataset and DataLoader for Machine Translation\n",
        "\n",
        "This section defines a PyTorch-compatible dataset and a collate function to prepare batches for training a transformer-based translation model.\n",
        "\n",
        "1. **`TranslationDataset` Class**:\n",
        "   - Inherits from `torch.utils.data.Dataset`.\n",
        "   - Takes lists of source (`src`) and target (`trg`) sentences along with their respective vocabularies.\n",
        "   - Implements `__len__` and `__getitem__`:\n",
        "     - `__getitem__` encodes each source and target sentence into tensors of token indices using `encode_sentence`.\n",
        "\n",
        "2. **`collate_fn` Function**:\n",
        "   - A custom function used to combine individual samples into a batch.\n",
        "   - Uses `pad_sequence` to pad sequences in the batch so they have equal lengths.\n",
        "   - Ensures correct padding values (`SRC_PAD_IDX` and `TRG_PAD_IDX`) are used for source and target batches.\n",
        "\n",
        "3. **`DataLoader` Setup**:\n",
        "   - Wraps the dataset in a `DataLoader` to enable efficient batch processing.\n",
        "   - Shuffles data and uses the custom `collate_fn` to handle variable-length sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef9bfd4b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:03.004980Z",
          "iopub.status.busy": "2025-07-06T08:47:03.004440Z",
          "iopub.status.idle": "2025-07-06T08:47:03.010667Z",
          "shell.execute_reply": "2025-07-06T08:47:03.009981Z",
          "shell.execute_reply.started": "2025-07-06T08:47:03.004955Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "ef9bfd4b"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src, trg, src_vocab, trg_vocab):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_seq = encode_sentence(self.src[idx], self.src_vocab)\n",
        "        trg_seq = encode_sentence(self.trg[idx], self.trg_vocab)\n",
        "        return src_seq, trg_seq\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX) # pads the seq to be the same length as the longest sequence\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=TRG_PAD_IDX)\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "dataset = TranslationDataset(english_sentences, french_sentences, src_vocab, trg_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd7dd493",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "bd7dd493"
      },
      "source": [
        "### Spit into train, val and test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0c003dfc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:05.698700Z",
          "iopub.status.busy": "2025-07-06T08:47:05.698212Z",
          "iopub.status.idle": "2025-07-06T08:47:05.715034Z",
          "shell.execute_reply": "2025-07-06T08:47:05.714514Z",
          "shell.execute_reply.started": "2025-07-06T08:47:05.698672Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "0c003dfc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Split ratios\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Total size\n",
        "dataset = TranslationDataset(english_sentences, french_sentences, src_vocab, trg_vocab)\n",
        "total_size = len(dataset)\n",
        "\n",
        "train_size = int(train_ratio * total_size)\n",
        "val_size = int(val_ratio * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "# Random split\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f863235",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "1f863235"
      },
      "source": [
        "### Positional Encoding Module\n",
        "\n",
        "This module adds **positional information** to input embeddings, enabling the transformer model to capture the order of tokens in a sequence.\n",
        "\n",
        "- Uses **sine and cosine functions** of different frequencies to generate position-dependent vectors.\n",
        "- Applies dropout for regularization.\n",
        "- The positional encodings are **precomputed and stored** as a buffer to avoid recomputation during training.\n",
        "\n",
        "The encoding is added to the input embeddings before they are passed into the transformer layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd57739",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "6bd57739"
      },
      "source": [
        "### Positional Encoding Formulas\n",
        "\n",
        "For each position `pos` and embedding dimension `i`, the positional encoding is defined as:\n",
        "\n",
        "- **Even dimensions** (i = 0, 2, 4, ...):\n",
        "\n",
        "$$\n",
        "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
        "$$\n",
        "\n",
        "- **Odd dimensions** (i = 1, 3, 5, ...):\n",
        "\n",
        "$$\n",
        "PE(pos, 2i + 1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### What this means:\n",
        "\n",
        "- Each position `pos` has a **unique encoding vector**\n",
        "- Each dimension `i` corresponds to a **different frequency**\n",
        "- Sine and cosine allow the model to **recover relative positions**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "170b9a17",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:08.183267Z",
          "iopub.status.busy": "2025-07-06T08:47:08.182583Z",
          "iopub.status.idle": "2025-07-06T08:47:08.188533Z",
          "shell.execute_reply": "2025-07-06T08:47:08.187910Z",
          "shell.execute_reply.started": "2025-07-06T08:47:08.183244Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "170b9a17"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pos_encoding = torch.zeros(max_len, d_model)\n",
        "        positions = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions * div_term) # for all 2i\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions * div_term) # for all 2i + 1\n",
        "\n",
        "        self.register_buffer('pe', pos_encoding.unsqueeze(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494dd7a4",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "494dd7a4"
      },
      "source": [
        "### Sequence-to-Sequence Transformer Model\n",
        "\n",
        "This class defines the core **Transformer-based encoder-decoder architecture** for sequence-to-sequence tasks such as machine translation.\n",
        "\n",
        "Key components:\n",
        "- **Embedding Layers**: Convert token indices from the source and target sequences into dense vector representations.\n",
        "- **Positional Encoding**: Adds position information to the embeddings to help the model understand word order.\n",
        "- **`nn.Transformer`**: The main transformer block containing multi-head attention, feedforward networks, and residual connections.\n",
        "  - Configurable number of encoder and decoder layers.\n",
        "- **Output Generator**: A linear layer that maps the transformer's output to the target vocabulary size for prediction.\n",
        "\n",
        "The `forward` method takes:\n",
        "- `src`, `tgt`: Source and target token sequences.\n",
        "- `src_mask`, `tgt_mask`: Masks to prevent attention to future tokens or pad tokens.\n",
        "- `src_padding_mask`, `tgt_padding_mask`: Masks to ignore padding in the attention mechanism.\n",
        "\n",
        "It returns the output logits for the target sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "353c3cfb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:10.053166Z",
          "iopub.status.busy": "2025-07-06T08:47:10.052691Z",
          "iopub.status.idle": "2025-07-06T08:47:10.058883Z",
          "shell.execute_reply": "2025-07-06T08:47:10.058113Z",
          "shell.execute_reply.started": "2025-07-06T08:47:10.053141Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "353c3cfb"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout)\n",
        "\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, None)\n",
        "        return self.generator(outs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a98b7a94",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "a98b7a94"
      },
      "source": [
        "### Mask Generation for Transformer\n",
        "\n",
        "Transformers require masks to control how tokens attend to one another during training, especially for tasks like language modeling and translation. This section defines two functions:\n",
        "\n",
        "1. **`generate_square_subsequent_mask(sz)`**:\n",
        "   - Creates an **upper triangular matrix** filled with `-inf` above the diagonal.\n",
        "   - Prevents the decoder from attending to future tokens during training (auto-regressive behavior).\n",
        "\n",
        "2. **`create_mask(src, tgt)`**:\n",
        "   - Generates the necessary masks for both source and target sequences:\n",
        "     - `src_mask`: A placeholder (all zeros) since the encoder attends to the full input.\n",
        "     - `tgt_mask`: Prevents the decoder from peeking ahead using `generate_square_subsequent_mask`.\n",
        "     - `src_padding_mask` and `tgt_padding_mask`: Identify padded positions in the input, so they can be ignored during attention computation.\n",
        "\n",
        "These masks are passed to the transformer during training and inference to ensure correct attention behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "208411e1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:12.415033Z",
          "iopub.status.busy": "2025-07-06T08:47:12.414773Z",
          "iopub.status.idle": "2025-07-06T08:47:12.420353Z",
          "shell.execute_reply": "2025-07-06T08:47:12.419633Z",
          "shell.execute_reply.started": "2025-07-06T08:47:12.415013Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "208411e1"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1) # keeps the elements above main diagonal inf\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    # Ensure tgt_mask and src_mask are both bool type\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).type(torch.bool) # mask future tokens (all -infs are True)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), dtype=torch.bool) # all tokens of src is known (all False)\n",
        "\n",
        "    # Padding masks for src and tgt (batch, seq_len)\n",
        "    src_padding_mask = (src == SRC_PAD_IDX).transpose(0, 1) # mask for pad tokens\n",
        "    tgt_padding_mask = (tgt == TRG_PAD_IDX).transpose(0, 1)\n",
        "\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1157a853",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "1157a853"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "371c1bb2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:47:32.442029Z",
          "iopub.status.busy": "2025-07-06T08:47:32.441311Z",
          "iopub.status.idle": "2025-07-06T08:51:25.942436Z",
          "shell.execute_reply": "2025-07-06T08:51:25.941667Z",
          "shell.execute_reply.started": "2025-07-06T08:47:32.442005Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "371c1bb2",
        "outputId": "7618ab1b-e1f5-46cb-dbca-27be17edb6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "Epoch 1 - Train: 100%|██████████| 3447/3447 [01:27<00:00, 39.47it/s]\n",
            "Epoch 1 - Val: 100%|██████████| 431/431 [00:04<00:00, 96.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "  Train Loss = 0.4568, Train Acc = 0.9037, Train BLEU = 75.57\n",
            "  Val   Loss = 0.0807, Val   Acc = 0.9745, Val   BLEU = 88.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 - Train: 100%|██████████| 3447/3447 [01:25<00:00, 40.40it/s]\n",
            "Epoch 2 - Val: 100%|██████████| 431/431 [00:04<00:00, 101.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2:\n",
            "  Train Loss = 0.0810, Train Acc = 0.9753, Train BLEU = 87.52\n",
            "  Val   Loss = 0.0537, Val   Acc = 0.9818, Val   BLEU = 90.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 - Train: 100%|██████████| 3447/3447 [01:25<00:00, 40.48it/s]\n",
            "Epoch 3 - Val: 100%|██████████| 431/431 [00:04<00:00, 88.87it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3:\n",
            "  Train Loss = 0.0572, Train Acc = 0.9809, Train BLEU = 88.74\n",
            "  Val   Loss = 0.0441, Val   Acc = 0.9848, Val   BLEU = 91.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Accuracy function\n",
        "def calculate_accuracy(logits, target):\n",
        "    pred = logits.argmax(dim=-1)\n",
        "    non_pad = target != TRG_PAD_IDX\n",
        "    correct = (pred == target) & non_pad\n",
        "    return correct.sum().item() / non_pad.sum().item()\n",
        "\n",
        "# Setup\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SRC_PAD_IDX = src_vocab['<pad>']\n",
        "SRC_SOS_IDX = src_vocab['<sos>']\n",
        "SRC_EOS_IDX = src_vocab['<eos>']\n",
        "\n",
        "TRG_PAD_IDX = trg_vocab['<pad>']\n",
        "TRG_SOS_IDX = trg_vocab['<sos>']\n",
        "TRG_EOS_IDX = trg_vocab['<eos>']\n",
        "\n",
        "SRC_VOCAB_SIZE = len(src_vocab)\n",
        "TGT_VOCAB_SIZE = len(trg_vocab)\n",
        "EMB_SIZE = 256\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                           NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "itos = trg_vocab.get_itos()\n",
        "smoothie = SmoothingFunction().method4 # this is for blue score\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_bleu = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for src, tgt in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Train\"):\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:-1, :]  # remove <eos>\n",
        "        tgt_out = tgt[1:, :]     # remove <sos>\n",
        "\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask.to(DEVICE), tgt_mask.to(DEVICE),\n",
        "                       src_pad_mask.to(DEVICE), tgt_pad_mask.to(DEVICE))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += calculate_accuracy(logits, tgt_out)\n",
        "\n",
        "        pred_tokens = logits.argmax(-1).transpose(0, 1).tolist()\n",
        "        target_tokens = tgt_out.transpose(0, 1).tolist()\n",
        "\n",
        "        for pred, ref in zip(pred_tokens, target_tokens):\n",
        "            pred_clean = [itos[tok] for tok in pred if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "            ref_clean = [itos[tok] for tok in ref if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "            if len(ref_clean) > 0 and len(pred_clean) > 0:\n",
        "                bleu = sentence_bleu([ref_clean], pred_clean, smoothing_function=smoothie)\n",
        "                total_bleu += bleu\n",
        "                total_samples += 1\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    avg_train_acc = total_acc / len(train_loader)\n",
        "    avg_train_bleu = (total_bleu / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "    # ------------------ Validation -------------------\n",
        "    model.eval()\n",
        "    val_loss, val_acc = 0, 0\n",
        "    val_bleu = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(val_loader, desc=f\"Epoch {epoch+1} - Val\"):\n",
        "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "            tgt_input = tgt[:-1, :]\n",
        "            tgt_out = tgt[1:, :]\n",
        "\n",
        "            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "            logits = model(src, tgt_input, src_mask.to(DEVICE), tgt_mask.to(DEVICE),\n",
        "                           src_pad_mask.to(DEVICE), tgt_pad_mask.to(DEVICE))\n",
        "\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "            val_loss += loss.item()\n",
        "            val_acc += calculate_accuracy(logits, tgt_out)\n",
        "\n",
        "            pred_tokens = logits.argmax(-1).transpose(0, 1).tolist()\n",
        "            target_tokens = tgt_out.transpose(0, 1).tolist()\n",
        "\n",
        "            for pred, ref in zip(pred_tokens, target_tokens):\n",
        "                pred_clean = [itos[tok] for tok in pred if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "                ref_clean = [itos[tok] for tok in ref if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "                if len(ref_clean) > 0 and len(pred_clean) > 0:\n",
        "                    bleu = sentence_bleu([ref_clean], pred_clean, smoothing_function=smoothie)\n",
        "                    val_bleu += bleu\n",
        "                    val_samples += 1\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_acc = val_acc / len(val_loader)\n",
        "    avg_val_bleu = (val_bleu / val_samples) * 100 if val_samples > 0 else 0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}:\")\n",
        "    print(f\"  Train Loss = {avg_train_loss:.4f}, Train Acc = {avg_train_acc:.4f}, Train BLEU = {avg_train_bleu:.2f}\")\n",
        "    print(f\"  Val   Loss = {avg_val_loss:.4f}, Val   Acc = {avg_val_acc:.4f}, Val   BLEU = {avg_val_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7c7ea9",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "ba7c7ea9"
      },
      "source": [
        "### Evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "114a97f5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:52:29.647204Z",
          "iopub.status.busy": "2025-07-06T08:52:29.646935Z",
          "iopub.status.idle": "2025-07-06T08:52:33.592277Z",
          "shell.execute_reply": "2025-07-06T08:52:33.591489Z",
          "shell.execute_reply.started": "2025-07-06T08:52:29.647184Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "114a97f5",
        "outputId": "f51aa9d2-761a-44f9-a960-43743aa33457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test Set: 100%|██████████| 431/431 [00:04<00:00, 101.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Test Loss: 0.0417, Test Accuracy: 0.9856, Test BLEU: 91.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "itos = trg_vocab.get_itos()\n",
        "\n",
        "# Test evaluation\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "model.eval()\n",
        "test_loss, test_acc = 0, 0\n",
        "total_bleu = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt in tqdm(test_loader, desc=\"Test Set\"):\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask.to(DEVICE), tgt_mask.to(DEVICE),\n",
        "                       src_pad_mask.to(DEVICE), tgt_pad_mask.to(DEVICE))\n",
        "\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        test_loss += loss.item()\n",
        "        test_acc += calculate_accuracy(logits, tgt_out)\n",
        "\n",
        "        # BLEU computation\n",
        "        pred_tokens = logits.argmax(-1).transpose(0, 1).tolist()\n",
        "        target_tokens = tgt_out.transpose(0, 1).tolist()\n",
        "\n",
        "        for pred, ref in zip(pred_tokens, target_tokens):\n",
        "            pred_clean = [itos[tok] for tok in pred if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "            ref_clean = [itos[tok] for tok in ref if tok not in {TRG_PAD_IDX, TRG_SOS_IDX, TRG_EOS_IDX}]\n",
        "            if len(ref_clean) > 0 and len(pred_clean) > 0:\n",
        "                bleu = sentence_bleu([ref_clean], pred_clean, smoothing_function=smoothie)\n",
        "                total_bleu += bleu\n",
        "                total_samples += 1\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "avg_test_acc = test_acc / len(test_loader)\n",
        "avg_test_bleu = (total_bleu / total_samples) * 100 if total_samples > 0 else 0\n",
        "\n",
        "print(f\"\\n📊 Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc:.4f}, Test BLEU: {avg_test_bleu:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0d0a88",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "1a0d0a88"
      },
      "source": [
        "### Checking on samples from test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "06340eb7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:52:37.045880Z",
          "iopub.status.busy": "2025-07-06T08:52:37.045620Z",
          "iopub.status.idle": "2025-07-06T08:52:37.052125Z",
          "shell.execute_reply": "2025-07-06T08:52:37.051383Z",
          "shell.execute_reply.started": "2025-07-06T08:52:37.045861Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "06340eb7"
      },
      "outputs": [],
      "source": [
        "def translate(model, sentence):\n",
        "    model.eval()\n",
        "    src = encode_sentence(sentence, src_vocab).unsqueeze(1).to(DEVICE)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = torch.zeros((num_tokens, num_tokens)).type(torch.bool).to(DEVICE)\n",
        "\n",
        "    memory = model.transformer.encoder(model.positional_encoding(model.src_tok_emb(src)), src_mask)\n",
        "    ys = torch.tensor([[trg_vocab['<sos>']]], dtype=torch.long).to(DEVICE)\n",
        "\n",
        "    for i in range(50):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(0)).to(DEVICE)\n",
        "        out = model.transformer.decoder(model.positional_encoding(model.tgt_tok_emb(ys)), memory, tgt_mask)\n",
        "        out = model.generator(out)\n",
        "        next_word = out[-1, 0].argmax().item()\n",
        "        ys = torch.cat([ys, torch.tensor([[next_word]], device=DEVICE)], dim=0)\n",
        "        if next_word == trg_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "    translated = [trg_vocab.get_itos()[token] for token in ys.squeeze()][1:-1]\n",
        "    return ' '.join(translated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6fde566d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-07-06T08:52:37.404451Z",
          "iopub.status.busy": "2025-07-06T08:52:37.403745Z",
          "iopub.status.idle": "2025-07-06T08:52:37.515210Z",
          "shell.execute_reply": "2025-07-06T08:52:37.514502Z",
          "shell.execute_reply.started": "2025-07-06T08:52:37.404427Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fde566d",
        "outputId": "adf5231c-d034-4cff-83fd-bd07086ea277"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Samples\n",
            "\n",
            "Sample 1:\n",
            "  English (src):  <sos> pears are my least liked fruit . <eos>\n",
            "  Predicted French (tgt): les poires sont mes fruits moins aimé .\n",
            "  GT French  (tgt):  <sos> les poires sont mes fruits moins aimé . <eos>\n",
            "\n",
            "Sample 2:\n",
            "  English (src):  <sos> france is snowy during september <eos>\n",
            "  Predicted French (tgt): la france est la neige au mois de septembre\n",
            "  GT French  (tgt):  <sos> la france est la neige au mois de septembre <eos>\n",
            "\n",
            "Sample 3:\n",
            "  English (src):  <sos> the peach is your least favorite fruit <eos>\n",
            "  Predicted French (tgt): la pêche est votre fruit préféré moins\n",
            "  GT French  (tgt):  <sos> la pêche est votre fruit préféré moins <eos>\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTest Set Samples\")\n",
        "\n",
        "for i in range(3):\n",
        "    src_sentence = test_dataset[i][0]\n",
        "    tgt_sentence = test_dataset[i][1]\n",
        "\n",
        "    # Decode token IDs back to words using vocab\n",
        "    src_text = ' '.join([src_vocab.get_itos()[tok] for tok in src_sentence if tok != SRC_PAD_IDX])\n",
        "    tgt_text = ' '.join([trg_vocab.get_itos()[tok] for tok in tgt_sentence if tok != TRG_PAD_IDX])\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"  English (src):  {src_text}\")\n",
        "    print(f\"  Predicted French (tgt): {translate(model, src_text)}\")\n",
        "    print(f\"  GT French  (tgt):  {tgt_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a93a7621",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "a93a7621"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0d97a534",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "0d97a534"
      },
      "source": [
        "####\n",
        "To-Do\n",
        "\n",
        "Q. Visualize results for other samples from the test set\n",
        "\n",
        "Q. Try different epoch values — e.g., 4, 5, 6 — and observe how the loss changes.\n",
        "\n",
        "Q. Experiment with different architecture configurations, such as changing the number of encoder and decoder layers.\n",
        "\n",
        "Q. Try for different batch sizes\n",
        "\n",
        "Q. We you find a different translation dataset try on that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca545f0e",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "ca545f0e"
      },
      "source": [
        "### Resources\n",
        "\n",
        "[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2410d43",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "d2410d43"
      },
      "source": [
        "### [Quiz](https://docs.google.com/forms/d/e/1FAIpQLScM0wn9cLVWCQvJemeOWlrK_RhGBmdeKOJBaqFkERFGmyqK3w/viewform?usp=dialog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92dd8416",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "completed"
        },
        "tags": [],
        "id": "92dd8416"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 203339,
          "isSourceIdPinned": false,
          "sourceId": 446960,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 151.831314,
      "end_time": "2025-07-06T09:19:21.984744",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-07-06T09:16:50.153430",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}